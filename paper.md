### **第一阶段：前期准备与文献调研 (1-2周)**

**你应该做的：**

1. **巩固基础知识：**
   - **深度学习推理优化：** 理解推理（Inference）和训练（Training）的区别，了解常见的推理优化技术（剪枝、量化、知识蒸馏等）。
   - **编译技术基础：** 了解传统编译器（如LLVM）的优化流程（IR， 中间表示；Pass， 优化遍）。重点学习**图优化**的概念。
   - **硬件知识：** 了解端侧设备（手机、嵌入式开发板）的硬件特点，特别是内存层次结构（缓存、主存）和计算单元（CPU/GPU/NPU）。
2. **深入文献调研：**
   - **核心框架论文：** 精读主流AI编译框架的论文或技术博客，如 **TVM**、**MLIR**、**TensorRT**、**MNN**、**NCNN** 等。重点关注它们的**算子融合**和**内存分配**策略。
   - **算子融合经典论文：** 搜索如 “Operator Fusion”、“Graph Optimization for DNN” 等关键词的论文。
   - **内存复用技术：** 了解 “静态内存规划”、“内存池技术”、“原地计算” 等概念。
   - **工具：** 熟练使用 Google Scholar、arXiv、GitHub。**做好文献笔记**。

**应向导师寻求的帮助：**

- **方向聚焦：** 请导师推荐该领域必读的**经典论文和最新前沿论文**，避免在浩如烟海的文献中迷失。
- **工具链建议：** 请导师建议1-2个最适合本课题、且社区活跃、易于上手的**编译框架**（如TVM通常是最佳起点）。
- **研究范围界定：** 和导师确认，是研究**通用策略**，还是针对**特定硬件**（如ARM CPU， Mali GPU）或**特定模型家族**（如Vision Transformer的轻量化版本、压缩后的CNN模型）。

### **第二阶段：环境搭建与现状分析 (2-3周)**

**你应该做的：**

1. **搭建实验环境：**
   - 安装选定的编译框架（如TVM）。
   - 在PC上搭建交叉编译环境，并准备一个端侧测试设备（如树莓派、装有特定SDK的手机，或使用QEMU模拟器）。
2. **基准测试与分析：**
   - **选择目标模型：** 选取2-3个经典的、经过剪枝/压缩的端侧模型（如MobileNetV2/V3、EfficientNet-Lite、TinyBERT）。
   - **现状分析：**
     - 使用框架工具（如TVM的 `relay.build` 和 `graph_executor`）**可视化计算图**，理解其结构。
     - **Profiling（性能剖析）：** 在端侧设备上运行未优化的模型，使用 profiling 工具（如TVM的 `profile_runtime`、Android的 `Perfetto`）收集数据。**关键指标**：每个算子的执行时间、内存占用峰值、内存申请释放次数。
     - **识别模式：** 根据文献，在计算图中人工识别常见的**可融合算子模式**（如 `Conv-BN-ReLU`、`Linear-BN`、`Conv-Add-ReLU`等）和**内存密集型算子**（如大矩阵的Transpose、Reshape等）。

**应向导师寻求的帮助：**

- **实验资源：** 确认实验室是否有**可用的端侧测试设备**（开发板、测试手机），以及获取方式。
- **调试支持：** 在环境搭建和初始模型跑通遇到困难时，及时请教导师或实验室的师兄师姐。

### **第三阶段：优化策略设计与实现 (4-6周) —— 毕设核心**

**你应该做的：**

1. **设计算子融合规则：**
   - 基于第二阶段的分析，制定具体的融合规则。例如：”将连续的同元素大小（element-wise）算子（ReLU, Sigmoid, Add）融合进前一个计算密集型算子（Conv, Gemm）中“。
   - 在你的编译框架中实现融合规则。在TVM中，这通常通过编写一个 **Relay Pass** 来实现。
   - 实现后，再次可视化计算图，**验证融合是否按预期发生**。
2. **设计内存复用策略：**
   - **策略1：内存共享：** 分析计算图中张量的生命周期。如果两个张量A和B的生命周期不重叠（即A释放后B才申请），可以让它们**共享同一块内存**。
   - **策略2：原地计算：** 对于某些算子（如 `ReLU`、`In-place Add`），可以让输出直接覆盖输入的内存，减少分配。
   - **策略设计：** 这可以建模为一个**图着色问题**或**线性规划问题**。作为本科毕设，可以优先实现基于**贪心算法**的简单内存分配器。
   - 在框架中实现：在TVM中，可以通过定制 `Schedule` 和 `Storage Rewrite` 等Pass来实现。
3. **迭代优化：** 设计-实现-测试-分析，这是一个循环过程。

**应向导师寻求的帮助：**

- **方案评审：** 将你设计的融合规则和内存复用策略**以书面形式（图表+伪代码）** 给导师看，请导师从**正确性、有效性和创新性**角度给予反馈。这是避免你走偏的关键一步。
- **关键技术指导：** 在实现Pass或算法遇到核心代码难题时，请求导师点拨思路。

### **第四阶段：实验验证与结果分析 (3-4周)**

**你应该做的：**

1. **设计严谨的实验：**
   - **对比基准：** 优化前的原始模型（Baseline）。
   - **实验组：** 仅启用算子融合、仅启用内存复用、两者都启用。
   - **评价指标：**
     - **速度：** 平均推理延迟（毫秒）、吞吐量（FPS）。
     - **内存：** 峰值内存占用（Peak Memory Usage）。
     - **理论指标：** 算子调用次数、内存分配次数（可通过日志或定制运行时收集）。
2. **全面测试：**
   - 在不同端侧设备（如果可能）上测试。
   - 在不同输入大小下测试。
   - 在选定的**具体任务**（如图像分类）上，不仅要测速度，还要确保优化**没有破坏模型精度**。
3. **深入分析结果：**
   - 制作清晰的**对比表格和图表**（柱状图、折线图）。
   - **分析性能提升的来源：** 是减少了内核启动开销？还是提升了缓存命中率？内存碎片减少了吗？
   - **讨论优化策略的局限性：** 在什么情况下（如模型结构特殊、算子类型特殊）优化效果不明显？为什么？

**应向导师寻求的帮助：**

- **实验设计审查：** 请导师看你的实验设计是否科学、全面，能否有力地支撑你的结论。
- **数据分析指导：** 如果某些实验结果与预期不符（如优化后反而变慢），请导师一起分析原因，这往往是产生深刻见解的好机会。

### **第五阶段：毕业论文撰写与答辩准备 (持续进行，最后2-3周集中)**

**你应该做的：**

1. **尽早开始写：** 从开题报告、文献综述开始，随着项目进展逐步完善各章节。
2. **论文结构建议：**
   - 绪论（背景、意义、问题陈述）
   - 相关工作（系统梳理现有编译框架的优化技术）
   - 系统设计与实现（你的分析、融合规则、内存策略，**配以清晰的架构图和流程图**）
   - 实验与评估（**实验环境、实验设计、结果与分析**是重中之重）
   - 总结与展望
3. **准备答辩：** 制作简洁有力的PPT，重点突出 **“问题 -> 你的方法 -> 验证结果”** 这条主线。准备好演示Demo。

**应向导师寻求的帮助：**

- **论文修改：** 将初稿、修改稿及时发给导师，请求对**结构、逻辑、技术表述的准确性**进行批改。
- **模拟答辩：** 请求导师或课题组组织预答辩，锻炼表达并获取改进意见。